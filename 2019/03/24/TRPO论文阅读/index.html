<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-cat.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-cat.png?v=7.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-cat.png?v=7.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.0.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="TRPO论文的推导过程。">
<meta name="keywords" content="论文阅读">
<meta property="og:type" content="article">
<meta property="og:title" content="TRPO论文阅读">
<meta property="og:url" content="http://yoursite.com/2019/03/24/TRPO论文阅读/index.html">
<meta property="og:site_name" content="Jairo">
<meta property="og:description" content="TRPO论文的推导过程。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/algo1.png">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/minimax.PNG">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/object.PNG">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/sample_path.png">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/gd.png">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/gradient.PNG">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/hessian.jpg">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/trpo.jpg">
<meta property="og:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/minimax.PNG">
<meta property="og:updated_time" content="2019-03-31T14:56:01.696Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TRPO论文阅读">
<meta name="twitter:description" content="TRPO论文的推导过程。">
<meta name="twitter:image" content="http://yoursite.com/2019/03/24/TRPO论文阅读/algo1.png">






  <link rel="canonical" href="http://yoursite.com/2019/03/24/TRPO论文阅读/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>TRPO论文阅读 | Jairo</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jairo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/TRPO论文阅读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jairo">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar_square.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jairo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TRPO论文阅读

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-24 23:08:40" itemprop="dateCreated datePublished" datetime="2019-03-24T23:08:40+08:00">2019-03-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-31 22:56:01" itemprop="dateModified" datetime="2019-03-31T22:56:01+08:00">2019-03-31</time>
              
            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/2019/03/24/TRPO论文阅读/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/24/TRPO论文阅读/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>TRPO论文的推导过程。</p>
<a id="more"></a>
<h1 id="Trust-Region-Policy-Optimization"><a href="#Trust-Region-Policy-Optimization" class="headerlink" title="Trust Region Policy Optimization"></a>Trust Region Policy Optimization</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">John Schulman</span><br></pre></td></tr></table></figure>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><ul>
<li>描述了一个用来<strong>优化策略</strong>的<strong>迭代过程</strong></li>
<li>这个过程是使得优化过程<strong>单调提高</strong>的</li>
<li>在对理论证明过程进行几处<strong>近似</strong>之后，提出一个实际算法<code>TRPO</code></li>
<li>该算法对于优化<strong>大规模非线性策略</strong>，例如神经网络，有较高的效率</li>
</ul>
<h1 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h1><ul>
<li>$S$——有限状态空间</li>
<li>$A$——有限动作空间</li>
<li>$P:S\times A\times S\to \mathbb{R}$——转移概率分布</li>
<li>$r:S\to \mathbb{R}$——奖励函数</li>
<li>$\rho_0:S\to \mathbb{R}$——初始状态$s_0$的概率分布</li>
<li>$\gamma \in (0,1)$——折扣因子</li>
<li>$\pi:S\times A \to [0,1]$——随机策略</li>
<li>$\eta(\pi)$——期望折扣回报  </li>
</ul>
<p>$$<br>\eta(\pi)=\mathbb{E}_{s_0,a_0,\ldots}\left[ \sum_{t=0}^{\infty}\gamma^tr(s_t) \right]<br>$$</p>
<p>其中：<br>$$<br>s_0\sim \rho_0(s_0),\quad a_t\sim \pi (a_t\mid s_t),\quad s_{t+1}\sim P(s_{t+1}\mid s_t,a_t)<br>$$<br>并定义如下三个量：</p>
<ol>
<li>状态-动作价值函数：</li>
</ol>
<p>$$<br>Q_\pi (s_t,a_t)=\mathbb{E}_{s_{t+1},a_{t+1},\ldots}\left[ \sum_{l=0}^{\infty}\gamma^l r(s_{t+l}) \right]<br>$$</p>
<ol start="2">
<li>价值函数：</li>
</ol>
<p>$$<br>V_{\pi}(s_t)=\mathbb{E}_{a_t,s_{t+1},\ldots} \left[\sum_{l=0}^{\infty}\gamma^l r(s_{t+l}) \right]<br>$$</p>
<ol start="3">
<li>优势函数：</li>
</ol>
<p>$$<br>A_\pi (s,a)=Q_\pi(s,a)-V_\pi(s)<br>$$</p>
<p>其中：<br>$$<br>a_t\sim \pi (a_t\mid s_t),\quad s_{t+1}\sim P(s_{t+1}\mid s_t,a_t)\quad for \quad t\ge 0<br>$$</p>
<ul>
<li>$V_\pi(s_t)$是状态$s_t$下，对所有可能动作$a$而言的期望价值</li>
<li>$Q_\pi(s_t,a_t)$是状态$s_t$下，当采取动作$a_t$时对应的价值</li>
<li>$A_\pi(s_t,a_t)$反映了状态$s_t$下，选择某一个动作对应的价值，和对于所有可能动作的期望价值的差</li>
</ul>
<h1 id="重写-eta-pi"><a href="#重写-eta-pi" class="headerlink" title="重写$\eta(\pi)$"></a>重写$\eta(\pi)$</h1><h2 id="——写成增量式"><a href="#——写成增量式" class="headerlink" title="——写成增量式"></a>——写成增量式</h2><p>我们希望每一次对策略$\pi$的更新，可以使得$\eta(\pi)$单调增大，能否将$\eta(\pi)$的表达式写成如下形式：<br>$$<br>\eta(\widetilde{\pi})=\eta(\pi)+(\ldots)<br>$$<br>这样的话，我们只需要考虑如何使得$(\ldots)\ge0$，变可保证$\eta(\pi)$单调增大  </p>
<h3 id="由-A-pi-s-a-重新定义-eta-pi-："><a href="#由-A-pi-s-a-重新定义-eta-pi-：" class="headerlink" title="由$A_\pi(s,a)$重新定义$\eta(\pi)$ ："></a>由$A_\pi(s,a)$重新定义$\eta(\pi)$ ：</h3><p>$$<br>\eta(\widetilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0,a_0,\ldots \sim \widetilde{\pi}}\left[ \sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]<br>$$</p>
<h3 id="proof-："><a href="#proof-：" class="headerlink" title="$proof$ ："></a>$proof$ ：</h3><p>考察两个策略$\pi$和$\widetilde{\pi}$。<br>首先注意到如下等式：<br>$$<br>A_\pi(s,a)=r(s)+\gamma V_\pi(s’)-V_\pi(s)<br>$$<br>于是有如下关系<br>$$<br>\begin{eqnarray}<br>&amp; &amp;\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]   \<br>&amp;=&amp;\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t (r(s_t)+\gamma V_\pi(s_{t+1})-V_\pi(s_t)) \right] \<br>&amp;=&amp;\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^{t+1}V_\pi(s_{t+1})- \sum_{t=0}^{\infty}\gamma^{t}V_\pi(s_{t})+\sum_{t=0}^{\infty}\gamma^t r({s_t}) \right] \<br>&amp;=&amp;\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ -V</em>\pi(s_0)+\sum_{t=0}^{\infty}\gamma^t r({s_t}) \right] \<br>\end{eqnarray}<br>$$<br>注意到<br>$$<br>\begin{eqnarray}<br>\left{<br>\begin{array}{ll}<br>\eta(\pi) &amp;=&amp; \mathbb{E}_{s_0,a_0,\ldots}\left[ \sum_{t=0}^{\infty}\gamma^tr(s_t) \right] \<br>V_{\pi}(s_t) &amp;=&amp; \mathbb{E}_{a_t,s_{t+1},\ldots} \left[\sum_{l=0}^{\infty}\gamma^l r(s_{t+l}) \right]<br>\end{array}<br>\right.<br>\Rightarrow V_\pi(s_0)=\eta(\pi)<br>\end{eqnarray}<br>$$<br>于是有<br>$$<br>\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]=-\eta(\pi)+\eta(\widetilde{\pi})<br>$$<br>即<br>$$<br>\eta(\widetilde{\pi})=\eta(\pi)+\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]<br>$$</p>
<h1 id="进一步改写-eta-pi"><a href="#进一步改写-eta-pi" class="headerlink" title="进一步改写$\eta(\pi)$"></a>进一步改写$\eta(\pi)$</h1><h2 id="——显式的写出-s-和-a"><a href="#——显式的写出-s-和-a" class="headerlink" title="——显式的写出$s$和$a$"></a>——显式的写出$s$和$a$</h2><p>$$<br>\eta(\widetilde{\pi})=\eta(\pi)+\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]<br>$$</p>
<p>可以看到我们成功的把对策略进行评价的折扣回报函数$\eta(\pi)$写成了$\eta(\widetilde{\pi})=\eta(\pi)+(\ldots)$的形式，于是我们要考虑这一项何时为正，为正时则要对策略进行更新。但这一表达式并没有给出太多信息，我们来把其中的状态$s$和动作$a$显式的表现出来：<br>$$<br>\begin{eqnarray}<br>\eta(\widetilde{\pi}) &amp;=&amp; \eta(\pi)+\mathbb{E}<em>{\tau \sim \widetilde{\pi}}\left[ \sum</em>{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]  \<br>\eta(\widetilde{\pi}) &amp;=&amp; \eta(\pi)+\sum_{t=0}^\infty  \sum_s P(s_t=s \mid \widetilde{\pi}) \sum_a \widetilde{\pi}(a_t=a \mid s_t) \gamma^t A_\pi(s_t,a_t) \qquad 期望是以概率为权重的加权和\<br>\eta(\widetilde{\pi}) &amp;=&amp; \eta(\pi)+ \sum_s \sum_{t=0}^\infty   \gamma^t  P(s_t=s \mid \widetilde{\pi})  \sum_a \widetilde{\pi}(a_t=a \mid s_t) A_\pi(s_t,a_t)  \qquad 调整各项的位置<br>\end{eqnarray}<br>$$<br>定义<strong>折扣状态访问概率</strong>：<br>$$<br>\rho_\pi (s)=P(s_0=s \mid \pi)+\gamma P(s_1=s \mid \pi)+\gamma^2 P(s_2=s \mid \pi)+\dots=\sum_{t=0}^\infty \gamma^t  P(s_t=s \mid \pi)<br>$$<br>表示策略$\pi$下，带有折扣因子的访问到状态$s$的概率（没有归一化），此时$\eta(\widetilde{\pi})$为：<br>$$<br>\begin{eqnarray}<br>\eta(\widetilde{\pi}) &amp;=&amp; \eta(\pi)+ \sum_s \sum_{t=0}^\infty \gamma^t  P(s_t=s \mid \widetilde{\pi})  \sum_a \widetilde{\pi}(a_t=a \mid s_t) A_\pi(s_t,a_t) \<br>\eta(\widetilde{\pi}) &amp;=&amp; \eta(\pi)+  \sum_s  \rho_\widetilde{\pi}(s) \sum_a \widetilde{\pi}(a \mid s) A_\pi(s,a)<br>\end{eqnarray}<br>$$<br>从这个式子可以看出，对于一个新策略$\widetilde{\pi}$如何判断其是否为更优的策略？就是对于在新策略$\widetilde{\pi}$下，对所有可能到达的状态$s$，考察其<strong>期望优势值</strong>，若有：<br>$$<br>\sum_a \widetilde{\pi}(a \mid s) A_\pi(s,a) \ge 0<br>$$<br>则说明$\widetilde{\pi}$为更优的策略，在所考察的状态$s$处，依据下式更新策略：<br>$$<br>\widetilde{\pi}(s)=\mathop{\arg\max}<em>a A</em>\pi(s,a)<br>$$<br>直到对于所有$\widetilde{\pi}$下可能到达的状态$s$，和状态$s$下所有可能采取的动作$a$都不再有正的$A_\pi(s,a)$，则说明收敛到最优策略。</p>
<h1 id="第一次近似"><a href="#第一次近似" class="headerlink" title="第一次近似"></a>第一次近似</h1><p>上一节推导出的表达式中，在新策略$\widetilde{\pi}$下，对所有可能到达的状态$s$，和状态$s$下所有可能采取的动作$a$，考察其<strong>期望优势值</strong>$A_\pi(s,a)$。对于一个参数化的策略$\pi_\theta(a \mid s)$，给定状态$s$输出动作$a$是很直接的，但若用其对状态采样，获得$\rho_{\pi}(s)$则是一个漫长的过程。考虑忽略<strong>折扣状态访问概率</strong>因策略更新而产生的变化，用$\rho_{\pi}(s)$替代$\rho_\widetilde{\pi}(s)$，于是便有了$\eta(\widetilde{\pi})$的近似表达式$L_\pi(\widetilde{\pi})$：<br>$$<br>L_\pi(\widetilde{\pi})=\eta(\pi)+  \sum_s  \rho_{\pi}(s) \sum_a \widetilde{\pi}(a \mid s) A_\pi(s,a)<br>$$<br>那么这样近似之后还可信吗？方便对比写出原式：<br>$$<br>\eta(\widetilde{\pi}) = \eta(\pi)+  \sum_s  \rho_\widetilde{\pi}(s) \sum_a \widetilde{\pi}(a \mid s) A_\pi(s,a)<br>$$<br>对于一个关于参数向量$\theta$可微的策略函数$\pi_\theta(a \mid s)$，有这样的关系：<br>$$<br>\begin{eqnarray}<br>\left{<br>\begin{array}{ll}<br>L_{\pi_{\theta_{old}}}(\pi_{\theta_{old}})=\eta(\pi_{\theta_{old}}) \<br>\nabla_\theta L_{\pi_{\theta_{old}}}(\pi_{\theta})\mid <em>{\theta=\theta</em>{old}}=\nabla_\theta \eta (\pi_{\theta})\mid_{\theta=\theta_{old}}<br>\end{array}<br>\right.<br>\end{eqnarray}<br>$$</p>
<h3 id="proof"><a href="#proof" class="headerlink" title="$proof:$"></a>$proof:$</h3><p>对第一个式子，因为策略并没有改变，故：<br>$$<br>L_{\pi_{\theta_{old}}}(\pi_{\theta_{old}})=\eta(\pi_{\theta_{old}})+\sum_s  \rho_{\pi_{\theta_{old}}}(s) \sum_a \pi_{\theta_{old}}(a \mid s) A_{\pi_{\theta_{old}}}(s,a)=\eta(\pi_{\theta_{old}})<br>$$<br>对第二个式子，分别来计算左右两项：<br>$$<br>\nabla_\theta L_{\pi_{\theta_{old}}}(\pi_{\theta})\mid <em>{\theta=\theta</em>{old}}=\sum_s  \rho_{\pi_{\theta_{old}}}(s) \sum_a \nabla_\theta\pi_{\theta}(a \mid s) A_{\pi_{\theta_{old}}}(s,a) \mid_{\theta=\theta_{old}} \<br>\nabla_\theta \eta (\pi_{\theta})\mid_{\theta=\theta_{old}}=\sum_s  \rho_{\pi_{\theta}}(s) \sum_a \nabla_\theta\pi_{\theta}(a \mid s) A_{\pi_{\theta_{old}}}(s,a) \mid_{\theta=\theta_{old}}<br>$$<br>实际操作中，$\sum_s  \rho_{\pi_{\theta}}(s)$是由样本信息得到的，当安照$\theta=\theta_{old}$，即$\pi_{\theta_{old}}$采样时，$\sum_s  \rho_{\pi_{\theta_{old}}}(s)=\sum_s  \rho_{\pi_{\theta}}(s)$，左右两项相等。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论:"></a>结论:</h3><p>表明对于函数$L_{\pi_{\theta_{old}}}(\pi_{\theta})$和$\eta (\pi_{\theta})$，在$\theta_{old}$处，对$\theta_{old}$更新足够小的一步，那么对$L_{\pi_{\theta_{old}}}(\pi_{\theta})$的提高相等于对$\eta (\pi_{\theta})$的提高。</p>
<h1 id="策略更新的定量描述"><a href="#策略更新的定量描述" class="headerlink" title="策略更新的定量描述"></a>策略更新的定量描述</h1><p>经过上面的讨论，现在就面临两个问题：</p>
<ol>
<li>如何衡量$\theta_{old}$与$\theta_{new}$之间的距离</li>
<li>更新策略后，会带来$\eta(\pi_{\theta})$多大的提升</li>
</ol>
<p>本文提出的衡量$\pi_{old}$和$\pi_{new}（此后用\pi_{old}表示\pi_{\theta_{old}}，用\pi_{new}表示\pi_{\theta_{new}}）$之间距离的方式为：$the\ total\  variation\  divergence$</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>$the\ total\  variation\  divergence$的表达式：<br>$$<br>D_{TV}(p||q)=\frac{1}{2}\sum_i \left| p_i-q_i \right|<br>$$<br>其中$p$和$q$为离散随机变量的概率函数。如果对连续随机变量，则$p$和$q$表示概率密度函数，需将求和改为积分。定义：<br>$$<br>D_{TV}^{max}(\pi_{old},\pi_{new})=\max_sD_{TV}(\pi_{old}(\cdot \mid s)\parallel \pi_{new}(\cdot \mid s))<br>$$</p>
<h3 id="重要不等式"><a href="#重要不等式" class="headerlink" title="重要不等式"></a>重要不等式</h3><p>有如下不等式关系：<br>$$<br>\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new})-\frac{4\epsilon \gamma}{(1-\gamma)^2}\alpha ^2<br>$$<br>其中：<br>$$<br>\epsilon=\max_{s,a}\left | A_\pi(s,a) \right| \<br>\alpha=D_{TV}^{max}(\pi_{old},\pi_{new})<br>$$<br>注意到$total\  variation\  divergence$ 和 $KL\ divergence$之间的关系：<br>$$<br>D_{TV}(p \parallel q)^2 \le D_{KL}(p \parallel q)<br>$$<br>令<br>$$<br>D_{KL}^{max}(\pi_{old},\pi_{new})=\max_sD_{TV}(\pi_{old}(\cdot \mid s)\parallel \pi_{new}(\cdot \mid s))<br>$$<br>于是经过变换得到了本文的重要不等式：<br>$$<br>\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new})-C\cdot D_{KL}^{max}(\pi_{old},\pi_{new})<br>$$<br>其中<br>$$<br>C=\frac{4\epsilon \gamma}{(1-\gamma)^2}<br>$$</p>
<h1 id="策略更新的方式"><a href="#策略更新的方式" class="headerlink" title="策略更新的方式"></a>策略更新的方式</h1><p>经过上面的讨论我们得出了一个重要的不等关系：<br>$$<br>\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new})-C\cdot D_{KL}^{max}(\pi_{old},\pi_{new})<br>$$<br>更改一下变量脚标：<br>此后以$\pi_i$表示当前策略，以$\pi_{i+1}$表示更新后的策略​<br>$$<br>\eta(\pi)\ge L_{\pi_{i}}(\pi)-C\cdot D_{KL}^{max}(\pi_{i},\pi)<br>$$<br>这是一个以$\pi$为变量的不等式，在$\pi=\pi_i$时取等</p>
<p>可以利用这一不等关系对参数化的策略函数进行更新。此不等式表明了$\eta(\pi)$的下限，我们用$M_i(\pi)$来表示此下限：<br>$$<br>M_i(\pi)=L_{\pi_i}(\pi)-C\cdot D_{KL}^{max}(\pi_{i},\pi)<br>$$<br>通过提升$\eta(\pi)$的下限$M_i(\pi)$，来带来$\eta(\pi)$的提升：</p>
<p>$$<br>\begin{eqnarray}<br>\left{<br>\begin{array}{ll}<br>\eta(\pi_{i+1})\ge M_i(\pi_{i+1}) \<br>\eta(\pi_i)=M_i(\pi_i)=L_{\pi_i}(\pi_i)<br>\end{array}<br>\right.<br>\Rightarrow<br>\eta(\pi_{i+1})-\eta(\pi_i)\ge M_i(\pi_{i+1})-M_i(\pi_i)<br>\end{eqnarray}<br>$$<br>这种更新是一种最小最大算法：<br><img src="/2019/03/24/TRPO论文阅读/algo1.png"><br><img src="/2019/03/24/TRPO论文阅读/minimax.PNG"></p>
<h1 id="第二次近似"><a href="#第二次近似" class="headerlink" title="第二次近似"></a>第二次近似</h1><p>以上我们得出了可以对策略迭代更新的算法，下面来考虑将其应用于实际问题。以下所讨论的策略$\pi$均是以向量$\theta$为参数的参数化的策略$\pi_{\theta}$，集中约定一下符号表示：</p>
<ul>
<li>$\eta(\theta):=\eta(\pi_\theta)$</li>
<li>$L_\theta(\widetilde{\theta}):=L_{\pi_{\theta}}(\pi_{\widetilde{\theta}})$</li>
<li>$D_{KL}(\theta \parallel \widetilde{\theta}):=D_{KL}(\pi_{\theta}\parallel \pi_{\widetilde{\theta}})$</li>
<li>$用\theta_{old}表示待更新的参数$  </li>
</ul>
<p>之前得到的不等式为：<br>$$<br>\eta(\theta)\ge L_{\theta_{old}}(\theta)-C\cdot D_{KL}^{max}(\theta_{old}\parallel \theta)<br>$$<br>更新$\theta$的过程是这样的：找到一个$\theta$使得不等号右边值最大，然后令$\theta_{old}=\theta$：<br>$$<br>\max_\theta \left[  L_{\theta_{old}}(\theta)-C\cdot D_{KL}^{max}(\theta_{old}\parallel \theta)\right]<br>$$<br>而这一更新方式在使用中有<strong>如下问题</strong>：</p>
<ul>
<li>如果使用理论推导出的不等式，系数$C=\frac{4\epsilon \gamma}{(1-\gamma)^2}$，那么更新步幅会很小，更新会很慢，而着眼于$D_{KL}^{max}$项，通过对这一项进行约束，将问题转换成有约束的优化问题，可以获得较大的更新步幅。这种对于新旧策略间差距的约束称为$trust \ region \ constraint$（这是TRPO区别于PPO的地方，PPO使用可调节大小的$C$）：</li>
</ul>
<p>$$<br>\max_\theta L_{\theta_{old}}(\theta) \<br>subject\ to \quad  D_{KL}^{max}(\theta_{old}\parallel \theta)\le \delta<br>$$</p>
<ul>
<li>注意到$D_{KL}^{max}$的定义式：</li>
</ul>
<p>$$<br>D_{KL}^{max}=\max_s D_{KL}(\pi_{\theta_{old}}(\cdot \mid s) \parallel \pi_{\theta}(\cdot \mid s) )<br>$$</p>
<p>​       可见$ D_{KL}^{max}(\theta_{old}\parallel \theta)\le \delta$这个约束是施加于所有状态的，要对每一个状态进行考察。</p>
<p>基于以上两点讨论，本文进行了<strong>第二次近似</strong>：</p>
<h4 id="用-average-KL-divergence-代替原来的-D-KL-max-约束项"><a href="#用-average-KL-divergence-代替原来的-D-KL-max-约束项" class="headerlink" title="用$average \ KL \ divergence$代替原来的$D_{KL}^{max}$约束项"></a>用$average \ KL \ divergence$代替原来的$D_{KL}^{max}$约束项</h4><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><p>$$<br>\bar{D}_{KL}^{\rho}(\theta_1,\theta_2):=\mathbb{E}<em>{s\sim \rho}\left[D</em>{KL}(\pi_{\theta_{1}}(\cdot \mid s) \parallel \pi_{\theta_{2}}(\cdot \mid s) )\right]<br>$$</p>
<p>这不是对所有可能到达的状态$s$中可取到的$D_{KL}$的最大值，而是考虑对于采样到的所有$s$的$D_{KL}$期望值。于是问题变为：<br>$$<br>\max_\theta L_{\theta_{old}}(\theta) \<br>subject \ to \quad \bar{D}<em>{KL}^{\rho</em>{\theta_{old}}}(\theta_{old},\theta) \le \delta<br>$$<br>这样近似之后，是否可行？<strong>在实验中发现，$\bar{D}<em>{KL}^{\rho}$和$D</em>{KL}^{max}$有着相似的表现</strong>。</p>
<h1 id="第三次近似"><a href="#第三次近似" class="headerlink" title="第三次近似"></a>第三次近似</h1><h2 id="——这一次近似了三处"><a href="#——这一次近似了三处" class="headerlink" title="——这一次近似了三处"></a>——这一次近似了三处</h2><p>接下来进一步走向实际使用，用$sample-based \ estimation$和$Monte Carlo simulation​$去估计<strong>目标函数</strong>和<strong>约束条件</strong>：<br>将目标函数展开：<br><img src="/2019/03/24/TRPO论文阅读/object.PNG"></p>
<ul>
<li>对$A_{\theta_{old}}(s,a)$的替换：</li>
</ul>
<p>$$<br>\begin{eqnarray}<br>\sum_a \pi_{\theta}(a \mid s) A_{\theta_{old}}(s,a) &amp;=&amp; \sum_a \pi_{\theta}(a \mid s)[Q_{\theta_{old}}(s,a)-V_{\theta_{old}}(s)]\<br>&amp;=&amp; \sum_a [\pi_{\theta}(a \mid s) Q_{\theta_{old}}(s,a)]- V_{\theta_{old}}(s) \sum_a \pi_{\theta}(a \mid s)    \<br>&amp;=&amp; \sum_a \pi_{\theta}(a \mid s) Q_{\theta_{old}}(s,a)-V_{\theta_{old}}(s)<br>\end{eqnarray}<br>$$</p>
<h1 id="优化目标的最终形态"><a href="#优化目标的最终形态" class="headerlink" title="优化目标的最终形态"></a>优化目标的最终形态</h1><p>$$<br>\max_{\theta}\mathbb{E}<em>{s\sim \rho</em>{\theta_{old}},a\sim q}\left[ \frac{\pi_{\theta}(a \mid s)}{q(a\mid s)}Q_{\theta_{old}(s,a)}\right]\<br>subject \ to \quad \mathbb{E}<em>{s\sim \rho</em>{\theta_{old}}}\left[ D_{KL}(\pi_{\theta_{old}}(\cdot \mid s)\parallel \pi_{\theta}(\cdot \mid s))\right] \le \delta<br>$$</p>
<p>在实际操作时：</p>
<ul>
<li>用样本均值$(sample \ averages)$替换期望$\mathbb{E}$</li>
<li>用实验估计$(empirical \ estimate)$替换$Q$值</li>
</ul>
<h1 id="采样估计方式"><a href="#采样估计方式" class="headerlink" title="采样估计方式"></a>采样估计方式</h1><ol>
<li><p>Single Path</p>
</li>
<li><p>Vine</p>
<img src="/2019/03/24/TRPO论文阅读/sample_path.png">
<h2 id="Single-Path"><a href="#Single-Path" class="headerlink" title="Single Path"></a>Single Path</h2></li>
<li><p>通过$s_0\sim \rho_0$选取一个$s_0$</p>
</li>
<li><p>由这个$s_0$出发，执行$\pi_{\theta_{old}}$。经过$T$步之后结束，得到一条轨迹：$s_0,a_0,s_1,a_1,\dots,s_{T-1},a_{T-1},s_T$</p>
</li>
<li><p>$q(a\mid s)$即为$\pi_{\theta_{old}}(a\mid s)$</p>
</li>
<li><p>$Q_{\theta_{old}(s,a)}$：在每一步$t$，计算从$t$到$T$之间的这一段轨迹的折扣回报总和</p>
</li>
</ol>
<h1 id="算法流程梳理"><a href="#算法流程梳理" class="headerlink" title="算法流程梳理"></a>算法流程梳理</h1><ol>
<li>使用$Single Path$或$Vine$的方式采集样本，即状态-动作对，估计$Q-Value$</li>
<li>将需要的量关于所有样本求平均，求出下式中需要估计出的值和约束项</li>
</ol>
<p>$$<br>\max_{\theta}\mathbb{E}<em>{s\sim \rho</em>{\theta_{old}},a\sim q}\left[ \frac{\pi_{\theta}(a \mid s)}{q(a\mid s)}Q_{\theta_{old}(s,a)}\right]\<br>subject \ to \quad \mathbb{E}<em>{s\sim \rho</em>{\theta_{old}}}\left[ D_{KL}(\pi_{\theta_{old}}(\cdot \mid s)\parallel \pi_{\theta}(\cdot \mid s))\right] \le \delta<br>$$</p>
<ol start="3">
<li>求解这一带有约束的最优化问题，去更新策略参数$\theta$</li>
</ol>
<h1 id="优化过程"><a href="#优化过程" class="headerlink" title="优化过程"></a>优化过程</h1><p>$$<br>\max_\theta L_{\theta_{old}}(\theta) \<br>subject \ to \quad \bar{D}<em>{KL}^{\rho</em>{\theta_{old}}}(\theta_{old},\theta) \le \delta<br>$$</p>
<p>用蒙特卡洛方法估计出目标函数与约束方程中的待估计值之后，来考虑如何解这一有约束的最优化问题。目标函数和约束方程都是关于策略参数$\theta$的函数，记作：<br>$$<br>\max_\theta l(\theta) \<br>subject \ to \  kl(\theta) \le \delta<br>$$<br>在$\theta_{old}$处，将 $l(\theta)$ 与$ kl(\theta)$做$Taylor$展开：<br>$l(\theta) \approx l(\theta_{old})+\nabla l(\theta_{old})^T(\theta-\theta_{old})+\frac{1}{2}(\theta-\theta_{old})^TH(l)(\theta_{old})(\theta-\theta_{old}) \qquad 第一项为常数；第三项极小\<br>\ \quad\approx g(\theta-\theta_{old})$<br>其中$g=\nabla l(\theta_{old})^T=\frac{\partial}{\partial \theta}l(\theta)\mid <em>{\theta=\theta</em>{old}}$<br>$kl(\theta) \approx kl(\theta_{old})+\nabla kl(\theta_{old})^T(\theta-\theta_{old})+\frac{1}{2}(\theta-\theta_{old})^T H(kl)(\theta_{old})(\theta-\theta_{old}) \qquad 第一项为0；第二项为0\<br>\ \ \ \quad\approx \frac{1}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old})$<br>其中$F=H(kl)(\theta_{old})=\frac{\partial^2}{\partial^2\theta}kl(\theta)\mid_{\theta=\theta_{old}}$<br>此时优化问题近似成：<br>$$<br>\max_\theta  g(\theta-\theta_{old}) \<br>subject \ to \  \frac{1}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old}) \le \delta<br>$$<br>其中$g=\nabla l(\theta_{old})^T=\frac{\partial}{\partial \theta}l(\theta)^T\mid <em>{\theta=\theta</em>{old}}$<br>$F=H(kl)(\theta_{old})=\frac{\partial^2}{\partial^2\theta}kl(\theta)\mid_{\theta=\theta_{old}}$  </p>
<h1 id="转换成无约束优化问题"><a href="#转换成无约束优化问题" class="headerlink" title="转换成无约束优化问题"></a>转换成无约束优化问题</h1><p>构建拉格朗日函数：<br>$$<br>L(\theta,\lambda)=g(\theta-\theta_{old})-\frac{\lambda}{2}\left[(\theta-\theta_{old})^TF(\theta-\theta_{old})-\delta \right]<br>$$<br>因为约束项为不等式，故还应该满足$KKT$条件：<br>$$<br>\begin{eqnarray}<br>\left{<br>\begin{array}{ll}<br>\frac{1}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old}) \le \delta \<br>\lambda \ge 0 \<br>\lambda \frac{1}{2}[(\theta-\theta_{old})^TF(\theta-\theta_{old})-\delta] = 0 \<br>\end{array}<br>\right. \Rightarrow \qquad \frac{1}{2}s^TFs=\delta<br>\end{eqnarray}<br>$$<br>其中$s$为参数$\theta$的更新方向$s=\theta-\theta_{old}$  </p>
<h2 id="至此我们就可以真正开始寻找函数的极值了"><a href="#至此我们就可以真正开始寻找函数的极值了" class="headerlink" title="至此我们就可以真正开始寻找函数的极值了"></a>至此我们就可以真正开始寻找函数的极值了</h2><h1 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h1><p>我们考虑这样的函数：标量函数$ f(x) \in \mathbb{R}$，其有着向量变元$x\in \mathbb{R}^n$，即：<br>$$<br>f(x):\mathbb{R}^n\to \mathbb{R}<br>$$<br>其梯度为（函数值增大最快的方向）：<br>$$<br>\nabla f(x)=\left&lt; \frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\ldots,\frac{\partial f}{\partial x_n} \right &gt;<br>$$<br>算法流程：</p>
<ol>
<li>随机选择初始点$x_0$</li>
<li>在当前点处计算函数梯度$v_i=\nabla f(x_i)$</li>
<li>沿负梯度方向进行点的更新$x_{i+1}=x_i-\alpha v_i$</li>
<li>不断进行1.、2.步直到函数值为0或足够小</li>
</ol>
<img src="/2019/03/24/TRPO论文阅读/gd.png">
<p>最速下降法的问题：</p>
<ol>
<li>当前点梯度反方向并不一定指向函数极值点</li>
<li>更新步长的选取$\alpha$要经过多次调整到合适的值</li>
</ol>
<h1 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h1><img src="/2019/03/24/TRPO论文阅读/gradient.PNG">
<p>先来看一个凸函数：<br>$$<br>f(x)=\frac{1}{2}x^TQx+qx+c<br>$$<br>其中$Q\in \mathbb{R}^{n\times n}$为对称正定矩阵，$q\in \mathbb{R}^n$<br>对于一个凸函数而言，任何局部极小值点都是该函数的全局极小点，若该函数是可微的，那么该函数极值点满足：<br>$$<br>\nabla f(x)=0<br>$$<br>于是有：<br>$$<br>Qx+b=0\quad \Rightarrow \quad Qx=-b \quad \Rightarrow\quad x=-Q^{-1}b<br>$$<br>这便直接找到$f(x)$的极值点。对比最速下降法，牛顿法利用了函数的二阶微分，即“梯度的梯度”，利用<strong>Hessian矩阵</strong>获得了更多的信息。当函数为凸函数时，可以直达函数极值点。<br>而对于<strong>更一般的函数</strong>，则可通过不断对其在迭代点进行<strong>二阶泰勒展开</strong>进行处理：<br>$$<br>f(x_i+\Delta x) \approx f(x_i)+\nabla f(x_i)\Delta x+\frac{1}{2}H[f(x_i)]\Delta x^2<br>$$<br>用此二次函数作为原函数$f(x)$的近似，求得此二次函数的极值点：<br>$$<br>x_{i+1}=x_i-H[f(x_i)]^{-1}\nabla f(x_i)<br>$$<br>然后再在$x_{i+1}$处进行泰勒展开得到新的近似函数，并继续寻找“近似函数”的极值点。迭代进行这一过程。<br>但牛顿法的局限性是很明显的：需要计算函数的Hessian矩阵——</p>
<ol>
<li>有着$O(n^2)$的存储空间要求和运算复杂度</li>
<li>尤其在神经网络中，二阶微分的传播需要的算法完全不同于$bp$算法</li>
</ol>
<h1 id="共轭梯度法"><a href="#共轭梯度法" class="headerlink" title="共轭梯度法"></a>共轭梯度法</h1><p>像最速下降法一样寻找一个更新方向，不奢求像牛顿法那样一步到位，只求方向选的科学、且有较大的更新步长。</p>
<h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>对一个凸函数，<br>$$<br>f(x)=\frac{1}{2}x^TQx+qx+c<br>$$<br>其中$Q\in \mathbb{R}^{n\times n}$为对称正定矩阵，$q\in \mathbb{R}^n$  </p>
<p>找到一组方向向量$d_1,\ldots,d_n \in \mathbb{R}^n$，依次按此方向组中的方向对迭代点$x_{i}\in \mathbb{R}^n$进行更新，对每一个方向$d_i$找到一个合适的步长$\lambda_i$，使得$f(x)$在该方向上取得最小值。 </p>
<p>有这样一个关键<strong>要求</strong>，在每一个新的更新方向$d_k$对迭代点$x_k$进行更新时，不能影响在之前方向$d_j(j\le k-1)$上的更新结果。也即$x_{k+1}$不仅使$f(x)$在$d_k$方向上取得最小值且在$d_j(j\le k-1)$方向上均保持最小值。  </p>
<p>如果能找到这样一组方向$d_1,\ldots,d_n \in \mathbb{R}^n$，那么在迭代$n$次之后将找到$f(x)$的全局极值点。</p>
<p>而矩阵<strong>$Q$的共轭方向组</strong>，正是我们寻找的这样一组方向。</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol>
<li>任意选择初始点$x_0$，初始更新方向$d_0=\nabla f(x_0)$</li>
<li>若$\nabla f(x_i)=0$，说明以找到极值点</li>
<li>若$\nabla f(x_i)\ne 0$，进行点的更新：$x_{i+1}=x_i+\lambda_i d_i$，<br>其中<br> $\lambda_i=\frac{-d_i^T\nabla f(x_i)}{d_i^TQd_i}$<br> $d_i=-\nabla f(x_i)+\gamma_{i-1}d_{i-1}$，这里$\gamma_{i-1}=\frac{d_{i-1}^TQ\nabla f(x_i)}{d_{i-1}^T Qd_{i-1}}$</li>
</ol>
<h2 id="扩展到一般函数"><a href="#扩展到一般函数" class="headerlink" title="扩展到一般函数"></a>扩展到一般函数</h2><p>如牛顿法一样，先在迭代点处，将函数进行二阶$Taylor$展开，用这一二次函数对原函数做一个相对好的近似，然后一步步到达该二次函数的极值点。之后进行迭代点的更新，再新的迭代点再做$Taylor$展开，寻找极值点。这样迭代的进行下去。</p>
<h2 id="对比牛顿法"><a href="#对比牛顿法" class="headerlink" title="对比牛顿法"></a>对比牛顿法</h2><p>这就是说，对于牛顿法直接解出的极值点$x=-Q^{-1}b$，共轭梯度法用了<strong>$n$步</strong>去走到该点。但每一步都方向明确，步长坚定，抵达每一个方向上的函数极值点，因为$Q-共轭方向组$的性质<strong>保证了</strong>之后的更新，不会再影响此方向上的更新结果。<br><strong>不用对Hessian矩阵求逆</strong>。但仍需对$Hessian$矩阵进行存储和计算。</p>
<h1 id="Hessian-Free"><a href="#Hessian-Free" class="headerlink" title="Hessian Free"></a>Hessian Free</h1><p>至此我们通过不断牺牲迭代速度来减小计算量和存储空间，而$Hessian Free$方法则是很聪明的一种处理方式可以让我们摆脱$Hessian$矩阵。<br>通过观察共轭梯度法的算法流程，发现$Hessian$矩阵总是以<strong>$matrix-vector \ products$</strong>的形式出现的，即$Hv$的形式。先来看$Hessian​$矩阵的表达式：<br><img src="/2019/03/24/TRPO论文阅读/hessian.jpg"><br>有以下关系：<br>$$<br>(Hv)<em>i=\sum</em>{j=1}^N \frac{\partial ^2 f}{\partial x_i x_j}(x)\cdot v_j=\nabla \frac{\partial f}{\partial x_i}(x)\cdot v<br>$$<br>而这正是函数$g=\frac{\partial f}{\partial x_i}$对于方向$v$的方向导数，根据方向导数的定义式：<br>$$<br>\nabla _v g=\lim _{\varepsilon \to 0}\frac{g(x+\varepsilon v)-g(x)}{\varepsilon}\approx \frac{g(x+\varepsilon v)-g(x)}{\varepsilon}<br>$$<br>于是得到：<br>$$<br>Hv \approx \frac{\nabla f(x+\varepsilon v)-\nabla f(x)}{\varepsilon}<br>$$<br>经过两次梯度反向传播，即可得到关于$Hessian$矩阵的$matrix-vector \ products$</p>
<h1 id="Line-search"><a href="#Line-search" class="headerlink" title="Line search"></a>Line search</h1><p>我们需要求极值的函数为此构造的拉格朗日函数：<br>$$<br>L(\theta,\lambda)=g(\theta-\theta_{old})-\frac{\lambda}{2}\left[(\theta-\theta_{old})^TF(\theta-\theta_{old})-\delta \right] \<br>constraint \ \frac{1}{2}s^TFs=\delta<br>$$<br>对每一次迭代，用采用$Hessian \ Free$处理方式的共轭梯度法，计算出由当前点指向当前极值点的向量$s_u=\frac{1}{\lambda}F^{-1}g$，为满足限制条件，需要对$s_u$进行修正：<br>$$<br>s=\sqrt{\frac{2\delta}{s_u^T F s_u}}s_u<br>$$<br>利用这一修正后的向量$s$进行线性搜索：<br>分别以向量$s,\frac{s}{2},\frac{s}{4},\ldots$与当前迭代点$x_i$相加，直到优化目标$\max_\theta L_{\theta_{old}}(\theta)​$有所提升。<br><img src="/2019/03/24/TRPO论文阅读/trpo.jpg"></p>
<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><p>写出折扣期望回报$\eta(\theta)$的增量形式：<br>$$<br>\eta(\widetilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0,a_0,\ldots \sim \widetilde{\pi}}\left[ \sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t) \right]<br>$$<br>写出$\eta(\theta)$的$local\ approximation$：<br>$$<br>L_\pi(\widetilde{\pi})=\eta(\pi)+  \sum_s  \rho_{\pi}(s) \sum_a \widetilde{\pi}(a \mid s) A_\pi(s,a)<br>$$<br>重要不等式：<br>$$<br>\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new})-C\cdot D_{KL}^{max}(\pi_{old},\pi_{new})<br>$$<br>其中<br>$$<br>C=\frac{4\epsilon \gamma}{(1-\gamma)^2}<br>$$<br><img src="/2019/03/24/TRPO论文阅读/minimax.PNG"><br>找到目标函数：<br>$$<br>\max_\theta \left[  L_{\theta_{old}}(\theta)-C\cdot \bar{D}<em>{KL}(\theta</em>{old}\parallel \theta)\right]<br>$$<br>近似为二次型：<br>$$<br>\max_{\theta}g(\theta-\theta_{old})-\frac{C}{2}(\theta-\theta_{old})^TF(\theta-\theta_{old})<br>$$<br>sulotion:<br>$$<br>\theta-\theta_{old}=\frac{1}{C}F^{-1}{g}<br>$$<br>用共轭梯度法（Hessian Free）去计算$F^{-1}g$</p>
<p>over!</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/论文阅读/" rel="tag"># 论文阅读</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/24/Python的若干基本操作/" rel="next" title="Python的若干基本操作">
                <i class="fa fa-chevron-left"></i> Python的若干基本操作
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/26/卷积网络中的图片预处理/" rel="prev" title="卷积网络中的图片预处理">
                卷积网络中的图片预处理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar_square.png" alt="Jairo">
            
              <p class="site-author-name" itemprop="name">Jairo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                  <a href="https://www.zhihu.com/people/Ja1r0/activities" title="知乎 &rarr; https://www.zhihu.com/people/Ja1r0/activities" rel="noopener" target="_blank">知乎</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                  <a href="https://weibo.com/u/5601451342" title="微博 &rarr; https://weibo.com/u/5601451342" rel="noopener" target="_blank">微博</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                  <a href="mailto:jairoyang@qq.com" title="邮件 &rarr; mailto:jairoyang@qq.com" rel="noopener" target="_blank">邮件</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Trust-Region-Policy-Optimization"><span class="nav-number">1.</span> <span class="nav-text">Trust Region Policy Optimization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#概述"><span class="nav-number">2.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#符号约定"><span class="nav-number">3.</span> <span class="nav-text">符号约定</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#重写-eta-pi"><span class="nav-number">4.</span> <span class="nav-text">重写$\eta(\pi)$</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#——写成增量式"><span class="nav-number">4.1.</span> <span class="nav-text">——写成增量式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#由-A-pi-s-a-重新定义-eta-pi-："><span class="nav-number">4.1.1.</span> <span class="nav-text">由$A_\pi(s,a)$重新定义$\eta(\pi)$ ：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#proof-："><span class="nav-number">4.1.2.</span> <span class="nav-text">$proof$ ：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#进一步改写-eta-pi"><span class="nav-number">5.</span> <span class="nav-text">进一步改写$\eta(\pi)$</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#——显式的写出-s-和-a"><span class="nav-number">5.1.</span> <span class="nav-text">——显式的写出$s$和$a$</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第一次近似"><span class="nav-number">6.</span> <span class="nav-text">第一次近似</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#proof"><span class="nav-number">6.0.1.</span> <span class="nav-text">$proof:$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-number">6.0.2.</span> <span class="nav-text">结论:</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#策略更新的定量描述"><span class="nav-number">7.</span> <span class="nav-text">策略更新的定量描述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义"><span class="nav-number">7.0.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重要不等式"><span class="nav-number">7.0.2.</span> <span class="nav-text">重要不等式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#策略更新的方式"><span class="nav-number">8.</span> <span class="nav-text">策略更新的方式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二次近似"><span class="nav-number">9.</span> <span class="nav-text">第二次近似</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#用-average-KL-divergence-代替原来的-D-KL-max-约束项"><span class="nav-number">9.0.0.1.</span> <span class="nav-text">用$average \ KL \ divergence$代替原来的$D_{KL}^{max}$约束项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定义-1"><span class="nav-number">9.1.</span> <span class="nav-text">定义</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第三次近似"><span class="nav-number">10.</span> <span class="nav-text">第三次近似</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#——这一次近似了三处"><span class="nav-number">10.1.</span> <span class="nav-text">——这一次近似了三处</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优化目标的最终形态"><span class="nav-number">11.</span> <span class="nav-text">优化目标的最终形态</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#采样估计方式"><span class="nav-number">12.</span> <span class="nav-text">采样估计方式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-Path"><span class="nav-number">12.1.</span> <span class="nav-text">Single Path</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#算法流程梳理"><span class="nav-number">13.</span> <span class="nav-text">算法流程梳理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#优化过程"><span class="nav-number">14.</span> <span class="nav-text">优化过程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#转换成无约束优化问题"><span class="nav-number">15.</span> <span class="nav-text">转换成无约束优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#至此我们就可以真正开始寻找函数的极值了"><span class="nav-number">15.1.</span> <span class="nav-text">至此我们就可以真正开始寻找函数的极值了</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#最速下降法"><span class="nav-number">16.</span> <span class="nav-text">最速下降法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#牛顿法"><span class="nav-number">17.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#共轭梯度法"><span class="nav-number">18.</span> <span class="nav-text">共轭梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本思路"><span class="nav-number">18.1.</span> <span class="nav-text">基本思路</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算法流程"><span class="nav-number">18.2.</span> <span class="nav-text">算法流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#扩展到一般函数"><span class="nav-number">18.3.</span> <span class="nav-text">扩展到一般函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对比牛顿法"><span class="nav-number">18.4.</span> <span class="nav-text">对比牛顿法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hessian-Free"><span class="nav-number">19.</span> <span class="nav-text">Hessian Free</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Line-search"><span class="nav-number">20.</span> <span class="nav-text">Line search</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#回顾"><span class="nav-number">21.</span> <span class="nav-text">回顾</span></a></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jairo</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.0.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.0"></script>

  <script src="/js/src/motion.js?v=7.0.0"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.0"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.0"></script>
<script src="/js/src/post-details.js?v=7.0.0"></script>



  


  <script src="/js/src/bootstrap.js?v=7.0.0"></script>


  
  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: '9IIUquKAB8dC33rTPetpM2J0-gzGzoHsz',
    appKey: '2VD0aGQxj9ligzFy97AIn5h9',
    placeholder: '让我听到你的声音~',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>




  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow-x: scroll;
  overflow-y: hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

  

  

  
<div id="hexo-helper-live2d">
  <canvas id="live2dcanvas" width="150" height="300"></canvas>
</div>
<style>
  #live2dcanvas{
    position: fixed;
    width: 150px;
    height: 300px;
    opacity:1;
    left: 0px;
    z-index: 999;
    pointer-events: none;
    bottom: -50px;
  }
</style>
<script type="text/javascript" src="/live2d/device.min.js"></script>
<script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    document.getElementById("live2dcanvas").style.width = '75px';
    document.getElementById("live2dcanvas").style.height = '150px';
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/hijiki.model.json", 0.5);});
})();
</script>

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
