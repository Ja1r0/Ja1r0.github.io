---
title: 深度学习中的正则化操作
date: 2019-05-04 23:34:56
tags:
---

<https://blog.csdn.net/u014380165/article/details/79810040>

<https://zhuanlan.zhihu.com/p/35005794>

<https://blog.csdn.net/qq_25737169/article/details/79048516>

<https://blog.csdn.net/liuxiao214/article/details/81037416>

<https://cloud.tencent.com/developer/article/1097463>

# 问题引出

​		对于神经网络，首先假设其优化算法为mini-batch SGD。在深度学习中，神经网络包含多个隐层，在模型训练的过程中，各个隐层的参数是不断在变化的。由于这种参数的变化，导致各个隐层的输入数据的分布也在不断变化。这一现象称为`Internal Covariate Shift`。batchnorm的目的是想让每个隐层的输入数据的分布稳定不变。

​		在用神经网络进行图像处理时，对整个模型的输入图像向量进行白化操作的话，会使神经网络较快的收敛。白化操作是指将图像数据转换为满足均值为零，单位方差的正态分布。以此为启发，在多个隐层的神经网络中，每一层的输出都是下一层的输入，batchnorm就是对每一层的输入数据都做一次简化的白化操作。对于每一个隐层来说，这种处理会减小输入数据的绝对差距，而保留相对差距。当这种相对差距满足我们想学习的分布时，batchnorm操作是有益的。而如果我们想学习的分布体现在数据的绝对差距中，那么则不适合使用batchnorm操作。

# 算法流程

